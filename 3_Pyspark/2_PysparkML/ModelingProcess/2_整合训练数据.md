<h1 align = "center">:rocket: 数据整合 :facepunch:</h1>

---
## 实例
```python
from udfs.SparkInit import *

# 仅关联5月数据
cond = col('stat_date') == '20170531'

dfs = [spark.table('finance.mls_member_lab_info_h').withColumnRenamed('data_date', 'stat_date')[cond],
       spark.table('fdm_dpa.mls_member_base_attr_h')[cond],
       spark.table('fdm_dpa.mls_member_base_date_info'),
       spark.table('fdm_dpa.mls_member_invest_action_h')[cond],
       spark.table('fdm_dpa.mls_member_trade_action_h')[cond],
       spark.table('fdm_dpa.mls_member_visit_action_h')[cond],
       spark.table('fdm_dpa.mls_member_value_attr_h')[cond].filter("eg_mem_lvl in ('161000000130', '161000000140')")]
df = dfs[6].cache()

def _remove(ls):
    ls.remove('acct_no')
    return ls

for i in range(1):
    name = _remove(list(set(dfs[i].columns).intersection(df.columns)))
    df = broadcast(df).join(dfs[i].drop(*name), 'acct_no', 'left_outer')

df.write.saveAsTable('fbidm.yuanjie_data_5', mode='overwrite')

train_data_5 = df
df = df_label[col('stat_date')=='5'].drop('stat_date').join(train_data_5, 'acct_no', 'left_outer')
df.write.saveAsTable('fbidm.yuanjie_slide_time_window_label', mode='overwrite')
```
---
